<h1 id="llm">Llm<a aria-hidden="true" class="anchor-heading icon-link" href="#llm"></a></h1>
<ul>
<li>
<p>LLM</p>
<ul>
<li>large language model</li>
</ul>
</li>
<li>
<p>multi-modal</p>
<ul>
<li>this means that LLMs go beyond utilizing text and language.</li>
<li>understanding images, audio.</li>
<li>works by using multiple encoders that create vector representation of given image, text respectively.</li>
<li>given some text representation of a concept, and a similar concept in image, the two respective encoders will map them to similar vectors, essentially allowing it to "speak the same language in the vector space"</li>
<li>once input in any modality is processed with the encoder, all we are left with vectors of certain dimension (probably something a human cannot understand intuitively), and now we are able to search in any direction, from any modality to any modality.</li>
</ul>
</li>
<li>
<p>contrastive pretraining</p>
<ul>
<li>two independantly trained models cannot map similar context to close vectors. When they are trained, respective models have to be aware of the other modes.
<ul>
<li>i.e. text and image encoders share indirect understanding of each other</li>
</ul>
</li>
<li>this works by having a pair of two representations in different modality, and learning to encode the pair as close as possible.</li>
<li>we also need negative pairs to "contrast" this learning.</li>
<li>given two positive pairs, we can switch the items in the pair to directly compose two negative pairs.</li>
</ul>
</li>
<li>
<p>transformer</p>
<ul>
<li>LSTMs were great but long ranged tasks like natural language was hard for them.</li>
<li>transformers, basically a set of encoder and decoder, has self-attention to tackle this issue</li>
</ul>
</li>
<li>
<p>transfer learning</p>
<ul>
<li>train a model to do a relatively easy task, and use that to do harder stuff</li>
</ul>
</li>
<li>
<p>prompts</p>
<ul>
<li>LLMs are trained with a simple idea of taking a sequence of text and output a sequence of text.</li>
<li>leveraging "good prompts" allows advanced usage of LLMs.</li>
<li>consists of <code>instructions</code>, <code>external information (context)</code>, <code>user input (query)</code>, <code>output indicator</code></li>
</ul>
</li>
<li>
<p>langchain</p>
<ul>
<li>provides various components that can be chained together to support advanced use cases with LLM</li>
<li>modules provided:
<ul>
<li>PromptTemplates</li>
<li>LLMs</li>
<li>Agents</li>
<li>Memory</li>
</ul>
</li>
</ul>
</li>
<li>
<p>prompt templates</p>
<ul>
<li>it's unlikely that we will hardcode instructiosn, context, and queries in a prompt</li>
<li>we will usually feed them dynamically, and prompt templates help us with that.</li>
<li>could be argued that we can just use f-strings for this, but the point here is to formalize the process of prompting.</li>
</ul>
</li>
<li>
<p>few-shot learning</p>
<ul>
<li>give a model a prompt that includes "a few examples" of what it should do</li>
</ul>
</li>
<li>
<p>memory</p>
<ul>
<li>by default llms are stateless, so it can't remember past conversations.</li>
<li>conversation chains allows us to keep a history of the conversation so that it could be used to infer context</li>
<li>conversational buffer memory
<ul>
<li>simply keep track of all the information fully</li>
<li>could slow calls down and easily hit token limit</li>
</ul>
</li>
<li>conversational summary memory
<ul>
<li>summarizes each interaction and store that summarization</li>
<li>take additional calls to summarize, summarization may be even longer than full text if text is short</li>
</ul>
</li>
<li>conversational buffer window memory
<ul>
<li>keep buffer, but for a short window</li>
<li>tackles token problem</li>
</ul>
</li>
<li>and so on...</li>
</ul>
</li>
<li>
<p>retrieval augmentation</p>
<ul>
<li>LLMs are frozen in time; unless they are trained with new data every time we need them to be aware of it, they will sit at a certain point in the past when they were trained (parametric knowledge).</li>
<li>instead of training again and again with new data, we instead give the external knowledge (source knowledge) to the LLM, and ask them to retrieve them.</li>
</ul>
</li>
<li>
<p>agents</p>
<ul>
<li>
<p>enabling "tools" for LLMs. Since LLMs are stuck with their trained data in the past, and struggle with some rather simple tasks that simple programs can solve easily, we instead let the LLM use "tools" to solve them, instead of trying to infer the answers themselves.</p>
</li>
<li>
<p>zero-shot ReAct</p>
<ul>
<li>no memory, consider one single interaction.</li>
</ul>
</li>
<li>
<p>conversational ReAct</p>
<ul>
<li>same as zero-shot, but with conversational memory.</li>
</ul>
</li>
<li>
<p>ReAct docstore</p>
<ul>
<li>same, but actively looks for information (lookup, search)</li>
</ul>
</li>
<li>
<p>self-ask with search</p>
<ul>
<li>has ability to search the web, and asks follow-up questions</li>
</ul>
</li>
</ul>
</li>
<li>
<p>tools</p>
<ul>
<li>objects that consume input and produces outputs.</li>
<li>generally input from LLM, and output to LLM</li>
<li>used by agents</li>
<li>can create custom tools that can be used by agents depending on the prompt and query as necessary</li>
</ul>
</li>
</ul>