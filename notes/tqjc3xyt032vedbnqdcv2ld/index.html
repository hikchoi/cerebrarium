<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>Llm</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Llm"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://cerebrarium.garden/notes/tqjc3xyt032vedbnqdcv2ld/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="5/21/2023"/><meta property="article:modified_time" content="5/21/2023"/><link rel="canonical" href="https://cerebrarium.garden/notes/tqjc3xyt032vedbnqdcv2ld/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6b338472289fe290.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/BQzlZRVSit1rkXJ_ur-Sy/_buildManifest.js" defer=""></script><script src="/_next/static/BQzlZRVSit1rkXJ_ur-Sy/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="llm">Llm<a aria-hidden="true" class="anchor-heading icon-link" href="#llm"></a></h1>
<ul>
<li>
<p>LLM</p>
<ul>
<li>large language model</li>
</ul>
</li>
<li>
<p>multi-modal</p>
<ul>
<li>this means that LLMs go beyond utilizing text and language.</li>
<li>understanding images, audio.</li>
<li>works by using multiple encoders that create vector representation of given image, text respectively.</li>
<li>given some text representation of a concept, and a similar concept in image, the two respective encoders will map them to similar vectors, essentially allowing it to "speak the same language in the vector space"</li>
<li>once input in any modality is processed with the encoder, all we are left with vectors of certain dimension (probably something a human cannot understand intuitively), and now we are able to search in any direction, from any modality to any modality.</li>
</ul>
</li>
<li>
<p>contrastive pretraining</p>
<ul>
<li>two independantly trained models cannot map similar context to close vectors. When they are trained, respective models have to be aware of the other modes.
<ul>
<li>i.e. text and image encoders share indirect understanding of each other</li>
</ul>
</li>
<li>this works by having a pair of two representations in different modality, and learning to encode the pair as close as possible.</li>
<li>we also need negative pairs to "contrast" this learning.</li>
<li>given two positive pairs, we can switch the items in the pair to directly compose two negative pairs.</li>
</ul>
</li>
<li>
<p>transformer</p>
<ul>
<li>LSTMs were great but long ranged tasks like natural language was hard for them.</li>
<li>transformers, basically a set of encoder and decoder, has self-attention to tackle this issue</li>
</ul>
</li>
<li>
<p>transfer learning</p>
<ul>
<li>train a model to do a relatively easy task, and use that to do harder stuff</li>
</ul>
</li>
<li>
<p>prompts</p>
<ul>
<li>LLMs are trained with a simple idea of taking a sequence of text and output a sequence of text.</li>
<li>leveraging "good prompts" allows advanced usage of LLMs.</li>
<li>consists of <code>instructions</code>, <code>external information (context)</code>, <code>user input (query)</code>, <code>output indicator</code></li>
</ul>
</li>
<li>
<p>langchain</p>
<ul>
<li>provides various components that can be chained together to support advanced use cases with LLM</li>
<li>modules provided:
<ul>
<li>PromptTemplates</li>
<li>LLMs</li>
<li>Agents</li>
<li>Memory</li>
</ul>
</li>
</ul>
</li>
<li>
<p>prompt templates</p>
<ul>
<li>it's unlikely that we will hardcode instructiosn, context, and queries in a prompt</li>
<li>we will usually feed them dynamically, and prompt templates help us with that.</li>
<li>could be argued that we can just use f-strings for this, but the point here is to formalize the process of prompting.</li>
</ul>
</li>
<li>
<p>few-shot learning</p>
<ul>
<li>give a model a prompt that includes "a few examples" of what it should do</li>
</ul>
</li>
<li>
<p>memory</p>
<ul>
<li>by default llms are stateless, so it can't remember past conversations.</li>
<li>conversation chains allows us to keep a history of the conversation so that it could be used to infer context</li>
<li>conversational buffer memory
<ul>
<li>simply keep track of all the information fully</li>
<li>could slow calls down and easily hit token limit</li>
</ul>
</li>
<li>conversational summary memory
<ul>
<li>summarizes each interaction and store that summarization</li>
<li>take additional calls to summarize, summarization may be even longer than full text if text is short</li>
</ul>
</li>
<li>conversational buffer window memory
<ul>
<li>keep buffer, but for a short window</li>
<li>tackles token problem</li>
</ul>
</li>
<li>and so on...</li>
</ul>
</li>
<li>
<p>retrieval augmentation</p>
<ul>
<li>LLMs are frozen in time; unless they are trained with new data every time we need them to be aware of it, they will sit at a certain point in the past when they were trained (parametric knowledge).</li>
<li>instead of training again and again with new data, we instead give the external knowledge (source knowledge) to the LLM, and ask them to retrieve them.</li>
</ul>
</li>
<li>
<p>agents</p>
<ul>
<li>
<p>enabling "tools" for LLMs. Since LLMs are stuck with their trained data in the past, and struggle with some rather simple tasks that simple programs can solve easily, we instead let the LLM use "tools" to solve them, instead of trying to infer the answers themselves.</p>
</li>
<li>
<p>zero-shot ReAct</p>
<ul>
<li>no memory, consider one single interaction.</li>
</ul>
</li>
<li>
<p>conversational ReAct</p>
<ul>
<li>same as zero-shot, but with conversational memory.</li>
</ul>
</li>
<li>
<p>ReAct docstore</p>
<ul>
<li>same, but actively looks for information (lookup, search)</li>
</ul>
</li>
<li>
<p>self-ask with search</p>
<ul>
<li>has ability to search the web, and asks follow-up questions</li>
</ul>
</li>
</ul>
</li>
<li>
<p>tools</p>
<ul>
<li>objects that consume input and produces outputs.</li>
<li>generally input from LLM, and output to LLM</li>
<li>used by agents</li>
<li>can create custom tools that can be used by agents depending on the prompt and query as necessary</li>
</ul>
</li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"tqjc3xyt032vedbnqdcv2ld","title":"Llm","desc":"","updated":1684650945504,"created":1684650945504,"custom":{},"fname":"ref.dev.llm","type":"note","vault":{"fsPath":"vault"},"contentHash":"701f9ba3fe67f1f4772c531dc6d7586a","links":[{"type":"linkCandidate","from":{"fname":"ref.dev.llm","id":"tqjc3xyt032vedbnqdcv2ld","vaultName":"vault"},"value":"conversational buffer memory","position":{"start":{"line":50,"column":16,"offset":2686},"end":{"line":50,"column":22,"offset":2692}},"to":{"fname":"buffer","vaultName":"vault"}},{"type":"linkCandidate","from":{"fname":"ref.dev.llm","id":"tqjc3xyt032vedbnqdcv2ld","vaultName":"vault"},"value":"conversational buffer window memory","position":{"start":{"line":56,"column":16,"offset":3031},"end":{"line":56,"column":22,"offset":3037}},"to":{"fname":"buffer","vaultName":"vault"}}],"anchors":{},"children":[],"parent":"85e77e30-28b1-40c4-a703-18c6efa566f4","data":{}},"body":"\u003ch1 id=\"llm\"\u003eLlm\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#llm\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eLLM\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elarge language model\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003emulti-modal\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethis means that LLMs go beyond utilizing text and language.\u003c/li\u003e\n\u003cli\u003eunderstanding images, audio.\u003c/li\u003e\n\u003cli\u003eworks by using multiple encoders that create vector representation of given image, text respectively.\u003c/li\u003e\n\u003cli\u003egiven some text representation of a concept, and a similar concept in image, the two respective encoders will map them to similar vectors, essentially allowing it to \"speak the same language in the vector space\"\u003c/li\u003e\n\u003cli\u003eonce input in any modality is processed with the encoder, all we are left with vectors of certain dimension (probably something a human cannot understand intuitively), and now we are able to search in any direction, from any modality to any modality.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003econtrastive pretraining\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etwo independantly trained models cannot map similar context to close vectors. When they are trained, respective models have to be aware of the other modes.\n\u003cul\u003e\n\u003cli\u003ei.e. text and image encoders share indirect understanding of each other\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ethis works by having a pair of two representations in different modality, and learning to encode the pair as close as possible.\u003c/li\u003e\n\u003cli\u003ewe also need negative pairs to \"contrast\" this learning.\u003c/li\u003e\n\u003cli\u003egiven two positive pairs, we can switch the items in the pair to directly compose two negative pairs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etransformer\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLSTMs were great but long ranged tasks like natural language was hard for them.\u003c/li\u003e\n\u003cli\u003etransformers, basically a set of encoder and decoder, has self-attention to tackle this issue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etransfer learning\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etrain a model to do a relatively easy task, and use that to do harder stuff\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eprompts\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLLMs are trained with a simple idea of taking a sequence of text and output a sequence of text.\u003c/li\u003e\n\u003cli\u003eleveraging \"good prompts\" allows advanced usage of LLMs.\u003c/li\u003e\n\u003cli\u003econsists of \u003ccode\u003einstructions\u003c/code\u003e, \u003ccode\u003eexternal information (context)\u003c/code\u003e, \u003ccode\u003euser input (query)\u003c/code\u003e, \u003ccode\u003eoutput indicator\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003elangchain\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eprovides various components that can be chained together to support advanced use cases with LLM\u003c/li\u003e\n\u003cli\u003emodules provided:\n\u003cul\u003e\n\u003cli\u003ePromptTemplates\u003c/li\u003e\n\u003cli\u003eLLMs\u003c/li\u003e\n\u003cli\u003eAgents\u003c/li\u003e\n\u003cli\u003eMemory\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eprompt templates\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit's unlikely that we will hardcode instructiosn, context, and queries in a prompt\u003c/li\u003e\n\u003cli\u003ewe will usually feed them dynamically, and prompt templates help us with that.\u003c/li\u003e\n\u003cli\u003ecould be argued that we can just use f-strings for this, but the point here is to formalize the process of prompting.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003efew-shot learning\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egive a model a prompt that includes \"a few examples\" of what it should do\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ememory\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eby default llms are stateless, so it can't remember past conversations.\u003c/li\u003e\n\u003cli\u003econversation chains allows us to keep a history of the conversation so that it could be used to infer context\u003c/li\u003e\n\u003cli\u003econversational buffer memory\n\u003cul\u003e\n\u003cli\u003esimply keep track of all the information fully\u003c/li\u003e\n\u003cli\u003ecould slow calls down and easily hit token limit\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003econversational summary memory\n\u003cul\u003e\n\u003cli\u003esummarizes each interaction and store that summarization\u003c/li\u003e\n\u003cli\u003etake additional calls to summarize, summarization may be even longer than full text if text is short\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003econversational buffer window memory\n\u003cul\u003e\n\u003cli\u003ekeep buffer, but for a short window\u003c/li\u003e\n\u003cli\u003etackles token problem\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eand so on...\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eretrieval augmentation\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLLMs are frozen in time; unless they are trained with new data every time we need them to be aware of it, they will sit at a certain point in the past when they were trained (parametric knowledge).\u003c/li\u003e\n\u003cli\u003einstead of training again and again with new data, we instead give the external knowledge (source knowledge) to the LLM, and ask them to retrieve them.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eagents\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eenabling \"tools\" for LLMs. Since LLMs are stuck with their trained data in the past, and struggle with some rather simple tasks that simple programs can solve easily, we instead let the LLM use \"tools\" to solve them, instead of trying to infer the answers themselves.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ezero-shot ReAct\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eno memory, consider one single interaction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003econversational ReAct\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esame as zero-shot, but with conversational memory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eReAct docstore\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esame, but actively looks for information (lookup, search)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eself-ask with search\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ehas ability to search the web, and asks follow-up questions\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etools\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eobjects that consume input and produces outputs.\u003c/li\u003e\n\u003cli\u003egenerally input from LLM, and output to LLM\u003c/li\u003e\n\u003cli\u003eused by agents\u003c/li\u003e\n\u003cli\u003ecan create custom tools that can be used by agents depending on the prompt and query as necessary\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"919a3a68-d78d-4403-aca5-ea6e722ca35b","title":"Cerebrarium","desc":"","updated":1640613661592,"created":1608971683570,"custom":{"nav_order":0,"permalink":"/"},"fname":"cerebrarium","type":"note","vault":{"fsPath":"vault"},"contentHash":"608b5806f5a9617a1356f40276dc6043","links":[{"from":{"fname":"ext","id":"Y_H6rBwXIinOO1QtrsNLV","vaultName":"vault"},"type":"backlink","position":{"start":{"line":40,"column":87,"offset":2834},"end":{"line":40,"column":102,"offset":2849},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"ext","id":"Y_H6rBwXIinOO1QtrsNLV","vaultName":"vault"},"type":"backlink","position":{"start":{"line":42,"column":99,"offset":3059},"end":{"line":42,"column":114,"offset":3074},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"inbox.do","id":"sKXyyWh2Ltz34Yor","vaultName":"vault"},"type":"backlink","position":{"start":{"line":6,"column":31,"offset":157},"end":{"line":6,"column":46,"offset":172},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"archive.proj.set-up-para","id":"3e110afd-f3d7-4e65-a6a2-68386e385830","vaultName":"vault"},"type":"backlink","position":{"start":{"line":5,"column":46,"offset":86},"end":{"line":5,"column":61,"offset":101},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"archive.daily.journal.2021.07.06","id":"YkY2M2OBrECDzy7tJUm8u","vaultName":"vault"},"type":"backlink","position":{"start":{"line":23,"column":19,"offset":1162},"end":{"line":23,"column":34,"offset":1177},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"archive.daily.journal.2021.07.06","id":"YkY2M2OBrECDzy7tJUm8u","vaultName":"vault"},"type":"backlink","position":{"start":{"line":26,"column":30,"offset":1483},"end":{"line":26,"column":45,"offset":1498},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"archive.daily.journal.2021.07.06","id":"YkY2M2OBrECDzy7tJUm8u","vaultName":"vault"},"type":"backlink","position":{"start":{"line":31,"column":19,"offset":1886},"end":{"line":31,"column":34,"offset":1901},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"archive.daily.journal.2021.07.10","id":"PiWkMf8_aK-TI3h1foE9t","vaultName":"vault"},"type":"backlink","position":{"start":{"line":7,"column":22,"offset":259},"end":{"line":7,"column":37,"offset":274},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"archive.daily.journal.2021.07.10","id":"PiWkMf8_aK-TI3h1foE9t","vaultName":"vault"},"type":"backlink","position":{"start":{"line":24,"column":39,"offset":1355},"end":{"line":24,"column":54,"offset":1370},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"archive.daily.journal.2021.07.11","id":"voYNbQ3CnDingwFbsRjsG","vaultName":"vault"},"type":"backlink","position":{"start":{"line":4,"column":22,"offset":102},"end":{"line":4,"column":37,"offset":117},"indent":[]},"value":"cerebrarium"},{"from":{"fname":"archive.daily.journal.2021.08.03","id":"gPOBTIoz1O5t9BrS","vaultName":"vault"},"type":"backlink","position":{"start":{"line":5,"column":114,"offset":270},"end":{"line":5,"column":129,"offset":285},"indent":[]},"value":"cerebrarium"}],"anchors":{"cerebrarium":{"type":"header","text":"Cerebrarium","value":"cerebrarium","line":10,"column":0,"depth":1}},"children":["f52825ef-7efe-4373-9784-255b9a058deb","815d5e1b-44ea-476a-868d-5fada4342924","jARgLS1htcLm5h3qdnuC8","65cb4031-d3d3-4e1a-a295-b96693df9d52","fc872c52-b139-4f32-80d5-9fcf05b41621","Jytev4B6USaFF48P","e6765163-38b4-42eb-a122-0d8ccca6ae22","ZtihdRyqFJMCdlVjMPuZj"],"parent":null,"data":{},"body":"\n![](/assets/images/2020-12-20-14-44-18.png)\n\n# Cerebrarium\n\n\u003e **Where I sow my cerebral seeds and tend to them.**\n\nThis is my current attempt to _**actively consume**_ information,\nand also try to travel back in time and excavate what I have been dumping into my head without any consideration.\n\nMost of the documents here will not make sense as they are working copies of my projects and ideas. This page is the entrypoint to my thoughts but there isn't a clearly sorted order of navigation. I simply decided to share these publicly to facilitate easy sharing and _**thinking out loud**_\n\nI plan to keep more polished and complete posts in my [main website](https://markhyunikchoi.com) and use this place as a nursery for my thoughts.\n","schema":{"schemaId":"cerebrarium","moduleId":"cerebrarium"}},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enableLinkCandidates":true,"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"none","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"vaultSelectionModeOnCreate":"alwaysPrompt","fuzzThreshold":0.1}},"randomNote":{},"insertNoteLink":{"aliasMode":"prompt","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.92.1","vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"","dateFormat":"y.MM.dd","addBehavior":"childOfCurrent"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"childOfCurrent"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableEditorDecorations":true,"workspaceVaultSyncMode":"sync","enableAutoFoldFrontmatter":false,"maxPreviewsCached":10,"maxNoteLength":204800,"enableUserTags":true,"enableHashTags":true,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["cerebrarium","ext","ref","proj"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"cerebrarium","description":"Personal knowledge space","author":"hikchoi"},"github":{"cname":"cerebrarium.garden","enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree","editRepository":"https://github.com/hikchoi/cerebrarium"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableHierarchyDisplay":false,"logoPath":"vault/assets/images/2020-12-20-14-44-18.png","siteFaviconPath":"vault/assets/images/favicon.ico","siteUrl":"https://cerebrarium.garden","siteIndex":"cerebrarium"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"tqjc3xyt032vedbnqdcv2ld"},"buildId":"BQzlZRVSit1rkXJ_ur-Sy","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>